\documentclass[a4paper]{report}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{lipsum} % genera testo fittizio
\usepackage{url}
\usepackage{amsmath}
\usepackage{braket}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage{geometry}
\usepackage{tikz}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\newcommand{\R}{\mathbb{R}}
\usepackage{bbold}
\usepackage{setspace}
\geometry{a4paper,top=2.5cm,bottom=2.5cm,left=3cm,right=2.5cm,%
	heightrounded,bindingoffset=5mm}
\numberwithin{equation}{chapter}
\setcounter{tocdepth}{3}
\begin{document}
	\author{Nunzia Cerrato, Giuseppe Catalano}
	\title{Report}
	\date{}
	\maketitle         
	
\section*{Problem 2}
Consider the $n \times n$ Wilkinson matrix

\begin{equation}\label{key}
	W_n = 
\begin{bmatrix}
	1 & 0  & 0  & \cdots  & 1  \\
	-1 & 1 & 0 & \cdots & 1 \\
	-1 & -1 & 1 & \cdots & 1 \\
	\vdots & \vdots & \vdots & \ddots & \vdots \\
	-1 & -1 & \cdots & -1 & 1 
\end{bmatrix}
\end{equation}

\noindent \textbf{(1)} We are interested to compute (by hand) the LU factorization of $W_{5}$, therefore to compute two $n \times n$ matrices, $L_5$ and $U_5$, that are, respectively, a unit lower triangular matrix and an upper triangular matrix that satisfy the identity $W_5 = L_5 U_5$. We start writing the expression of $W_5$:
\begin{equation}\label{key}
	W_5 = 
	\begin{bmatrix}
		1 & 0  & 0  & 0  & 1  \\
		-1 & 1 & 0 & 0 & 1 \\
		-1 & -1 & 1 & 0 & 1 \\
		-1 & -1 & -1 & 1 & 1 \\
		-1 & -1 & -1 & -1 & 1 
	\end{bmatrix},
\end{equation}
now we compute $\textbf{m}_1$:
\begin{equation}\label{key}
	\textbf{m}_1 = 
	\begin{bmatrix}
		0\\
		{w_{21}}/{w_{11}}\\
		{w_{31}}/{w_{11}}\\
		{w_{41}}/{w_{11}}\\
		{w_{51}}/{w_{11}}
	\end{bmatrix}=
	\begin{bmatrix}
	0\\
	-1\\
	-1\\
	-1\\
	-1
\end{bmatrix}.
\end{equation}
Defining $\textbf{e}_i$ as the vectors with $1$ in the $i-th$ element and $0$ otherwise, we can compute:
\begin{equation}\label{key}
	M_1=\mathcal{I}_5 - \textbf{m}_1 \textbf{e}_1^T =
		\begin{bmatrix}
		1 & 0  & 0  & 0  & 0  \\
		1 & 1 & 0 & 0 & 0\\
		1 & 0 & 1 & 0 & 0 \\
		1 & 0 & 0 & 1 & 0 \\
		1 & 0 & 0 & 0 & 1 
	\end{bmatrix}.
\end{equation}
Using the expression of $M_1$, we can compute $W_5^{(1)}$:
\begin{equation}\label{key}
	W_5^{(1)} = M_1 W_5 = 
	\begin{bmatrix}
		1 & 0  & 0  & 0  & 1  \\
		0 & 1 & 0 & 0 & 2 \\
		0 & -1 & 1 & 0 & 2 \\
		0 & -1 & -1 & 1 & 2 \\
		0 & -1 & -1 & -1 & 2 
	\end{bmatrix}.
\end{equation}
The second iteration proceeds in a similar way:
\begin{equation}\label{key}
	\textbf{m}_2 = 
	\begin{bmatrix}
		0\\
		0\\
		{w^{(1)}_{32}}/{w^{(1)}_{22}}\\
		{w^{(1)}_{42}}/{w^{(1)}_{22}}\\
		{w^{(1)}_{52}}/{w^{(1)}_{22}}
	\end{bmatrix}=
	\begin{bmatrix}
		0\\
		0\\
		-1\\
		-1\\
		-1
	\end{bmatrix},
\end{equation}

\begin{equation}\label{key}
	M_2=\mathcal{I}_5 - \textbf{m}_2\textbf{e}_2^T =
	\begin{bmatrix}
		1 & 0  & 0  & 0  & 0  \\
		0 & 1 & 0 & 0 & 0\\
		0 & 1 & 1 & 0 & 0 \\
		0 & 1 & 0 & 1 & 0 \\
		0 & 1 & 0 & 0 & 1 
	\end{bmatrix}.
\end{equation}

\noindent Using the expression of $M_2$, we can compute $W_5^{(2)}$:
\begin{equation}\label{key}
	W_5^{(2)} = M_2 W_5^{(1)} = M_2 M_1 W_5 =
	\begin{bmatrix}
		1 & 0  & 0  & 0  & 1  \\
		0 & 1 & 0 & 0 & 2 \\
		0 & 0 & 1 & 0 & 4 \\
		0 & 0 & -1 & 1 & 4 \\
		0 & 0 & -1 & -1 & 4 
	\end{bmatrix}.
\end{equation}
	
\noindent Now we start the third iteration:
\begin{equation}\label{key}
	\textbf{m}_3 = 
	\begin{bmatrix}
		0\\
		0\\
		0\\
		{w^{(2)}_{43}}/{w^{(2)}_{33}}\\
		{w^{(2)}_{53}}/{w^{(2)}_{33}}
	\end{bmatrix}=
	\begin{bmatrix}
		0\\
		0\\
		0\\
		-1\\
		-1
	\end{bmatrix},
\end{equation}
	
\begin{equation}\label{key}
	M_3=\mathcal{I}_5 - \textbf{m}_3\textbf{e}_3^T =
	\begin{bmatrix}
		1 & 0  & 0  & 0  & 0  \\
		0 & 1 & 0 & 0 & 0\\
		0 & 0 & 1 & 0 & 0 \\
		0 & 0 & 1 & 1 & 0 \\
		0 & 0 & 1 & 0 & 1 
	\end{bmatrix}.
\end{equation}
	
\noindent Using the expression of $M_3$, we can compute $W_5^{(3)}$:
\begin{equation}\label{key}
	W_5^{(3)} = M_3 W_5^{(2)} = M_3 M_2 M_1 W_5 =
	\begin{bmatrix}
		1 & 0  & 0  & 0  & 1  \\
		0 & 1 & 0 & 0 & 2 \\
		0 & 0 & 1 & 0 & 4 \\
		0 & 0 & 0 & 1 & 8 \\
		0 & 0 & 0 & -1 & 8 
	\end{bmatrix}.
\end{equation}

\noindent Similarly, we can perform the last iteration:
\begin{equation}\label{key}
	\textbf{m}_4 = 
	\begin{bmatrix}
		0\\
		0\\
		0\\
		0\\
		{w^{(2)}_{54}}/{w^{(2)}_{44}}
	\end{bmatrix}=
	\begin{bmatrix}
		0\\
		0\\
		0\\
		0\\
		-1
	\end{bmatrix},
\end{equation}

\begin{equation}\label{key}
	M_4=\mathcal{I}_5 - \textbf{m}_4\textbf{e}_4^T =
	\begin{bmatrix}
		1 & 0  & 0  & 0  & 0  \\
		0 & 1 & 0 & 0 & 0\\
		0 & 0 & 1 & 0 & 0 \\
		0 & 0 & 0 & 1 & 0 \\
		0 & 0 & 0 & 1 & 1 
	\end{bmatrix}.
\end{equation}

\noindent Using the expression of $M_4$, we can compute $W_5^{(4)}$:
\begin{equation}\label{key}
	W_5^{(4)} = M_4 W_5^{(3)} = M_4 M_3 M_2 M_1 W_5 =
	\begin{bmatrix}
		1 & 0  & 0  & 0  & 1 \\
		0 & 1 & 0 & 0 & 2 \\
		0 & 0 & 1 & 0 & 4 \\
		0 & 0 & 0 & 1 & 8 \\
		0 & 0 & 0 & 0 & 16 
	\end{bmatrix} = U_5.
\end{equation}
Since this last matrix is upper triangular, we call it $U_{5}$ and define $L^{-1}_{5} \coloneqq M_4 M_3 M_2 M_1$, such that $L^{-1}_{5}W_{5}= U_{5}$. We can get $L_5$ from $L_5 = M_1^{-1} M_2^{-1} M_3^{-1} M_4^{-1}$ and knowing that $M_i^{-1} = \mathcal{I}_5 + \textbf{m}_i \textbf{e}_i^T$:
\begin{equation}\label{key}
	L_5 = M_1^{-1} M_2^{-1} M_3^{-1} M_4^{-1} = 
	\begin{bmatrix}
		1 & 0  & 0  & 0  & 0  \\
		-1 & 1 & 0 & 0 & 0 \\
		-1 & -1 & 1 & 0 & 0 \\
		-1 & -1 & -1 & 1 & 0 \\
		-1 & -1 & -1 & -1 & 1 
	\end{bmatrix}.
\end{equation}
	
\noindent \textbf{(2)} It is possible to guess the LU factorization of $W_n = L_n U_n$:
\begin{equation}\label{key}
	L_n =
\begin{bmatrix}
	1 & 0  & \cdots  & \cdots  & 0 \\
	-1 & 1 & \ddots & \  & \vdots \\
	\vdots & \ddots & 1 & \ddots & \vdots \\
	\vdots & \  & \ddots & \ddots & 0 \\
	-1 & \cdots & \cdots & -1 & 1 
\end{bmatrix},\ \ \ 
U_n = 
\begin{bmatrix}
	1 & 0  & \cdots  & 0  & 2^0 \\
	0& 1 & \ddots & \vdots & 2^1 \\
	\vdots & \ddots & \ddots & 0 & 2^2 \\
	\vdots & \  & \ddots & 1 & \vdots \\
	0 & \cdots & \cdots & 0 & 2^{n-1}
\end{bmatrix}
\end{equation}
	
	
\noindent \textbf{(3)} In the following, we report the function that generates the $n \times n $ Wilkinson matrix.\\
*** INSERIRE CODICE (funzione wilkin(n) ) ***\\


\noindent 

\noindent \textbf{(4-5-6)} In the following, we report the code that performs the numerical experiment for each $n=2,\dots,60$.\\

*** INSERIRE CODICE (funzione check\_when\_lufact\_W\_fails) ***\\


\noindent We have seen that the largest value of $n$ for which $W_n \textbf{x} = \textbf{b}$ can be solved accurately is $54$, that means that for $n=55$ the program returns an inaccurate value for the solution $\textbf{x}$. In particular, instead of computing the value $\textbf{x} = \textbf{e} = [1,\dots,1]^T$, it computes $\tilde{\textbf{x}} = [1,\dots,1,0,1]^T$. In other words we have:
\begin{equation}\label{key}
	\tilde{\textbf{x}}_{54} = 0 \neq \textbf{e}_{54} = 1.
\end{equation}
In order to understand the motivation uder this behavior, we verified that the matrices $L_{55}$ and $U_{55}$ were computed accurately. In the following is reported the code that verifies this computation.\\

*** INSERIRE CODICE (funzioni expected\_LU\_wilkin, compute\_error\_lufact\_W) ***\\


\noindent  Once verified that the matrices $L_{55}$ and $U_{55}$ are correct, we know that the problem must be in the calculation of the forward and backward substitution. We recall that having performed the LU factorization of the matrix $W_{55}$ allows us to solve the system $W_{55} \textbf{x} = \textbf{b}_{55}$ by solving (in order) two linear systems with forward and backward substitutions:
\begin{equation}\label{key}
	\left\lbrace \begin{split}
		& L \textbf{y} = \textbf{b} \\
		& U \textbf{x} = \textbf{y}
	\end{split} \right. 
\end{equation}
where we named $L_{55} = L$, $U_{55} = U$ and $\textbf{b}_{55} = \textbf{b}$ for clarity.
It can be verified that the analytical solution to the first system is $\textbf{y} = [2^0 + 1, 2^1 + 1, \dots,2^{n-2}+1, 2^{n-1}]$. At this point, a very important observation arises: if we consider $\textbf{y}_{54}$ = $2^{53} + 1 $, we may notice that, when the sum $2^{53} + 1$ is performed in double precision, the result is $2^{53}$. This happens because $1$ is less than the machine precision associated to the number $2^{53}$ in double precision:
\begin{equation}\label{key}
	1< 2^{53} \cdot \varepsilon \simeq 2^{53} \cdot 2.22 \cdot 10^{-16} \simeq 0.9 \cdot 10^{16} \cdot 2.22 \cdot 10^{-16} \simeq 2.
\end{equation}
After having computed $\textbf{y}$, we can compute the solution of the second system starting from the bottom:
\begin{equation}\label{key}
	\textbf{x}_{55} = \textbf{y}_{55}/U_{55,55} = \frac{2^{54}}{2^{54}} = 1,
\end{equation}
so far so good. Now we update the vector $\textbf{y}$ as follows:
\begin{equation}\label{key}
	\textbf{y}^{(1)} = \textbf{y} - \textbf{x}_{55} \textbf{u}_{55},
\end{equation}
where $\textbf{u}_{55}$ is the $55-th$ and last column of $U$. The $54-th$ component of $\textbf{y}^{(1)}$ will be:
\begin{equation}\label{key}
	[\textbf{y}^{(1)}]_{54} = [\textbf{y}]_{54} - \textbf{x}_{55} U_{54,55} = 2^{53} + 1 - 2^{53},
\end{equation}
but, since in double precision we have $ 2^{53} + 1 = 2^{53}$, the returned value of  $[\textbf{y}^{(1)}]_{54}$ will be $0$ and not $1$. If we execute the same algorithm for bigger values of $n$, the number of elements of the solution vector that will be miscalculated will grow, starting from the penultimate element of the vector $\textbf{x}$. It is worth noting that the last element of the solution is not affected by this kind of error because it is computed via the operation:
\begin{equation}\label{key}
	\textbf{x}_{n} = \textbf{y}_{n}/U_{n,n} = \frac{2^{n-1}}{2^{n-1}}
\end{equation}
that does not lead to catastrophical cancellations. These considerations imply that the GEPP algorithm is not backward stable when the input is a Wilkinson matrix. However, it is important to say that this is a very artificial example where the growth factor $\gamma$ assumes the maximum possible value, i.e. $\gamma = 2^{n-1}$. However, in most cases, the GEPP algorithm is backward stable.\\
The just described behavior can be observed from the execution of the function reported here.\\

*** INSERIRE CODICE ***\\

\section*{Problem 3}
Suppose that $A \in \R^{n \times n}$ is a nonsingular matrix and the LU factorization of $A$ exists and has been computed. Consider two given vectors $\textbf{u},\textbf{v}\in \R^n$, we can define the matrix $\tilde{A} = A + \textbf{u}\textbf{v}^T$\\
\textbf{(1a)} Prove that $\tilde{A}$ is nonsingular if and only if $\textbf{v}^TA^{-1}\textbf{u} \neq 1$.\\
Proof: We start proving that $\det(\tilde{A})\neq 0 $ implies that $\textbf{v}^TA^{-1}\textbf{u} \neq 1$. We can choose an orthonormal basis $\mathcal{B} =\{\mathbf{e}_i\}_{i=1,\dots,n}$ of $\R^n$ such that $\mathbf{u} = \alpha_1 \mathbf{e}_1$ and $\mathbf{v} = \beta_1 \mathbf{e}_1 + \beta_2 \mathbf{e}_2$ with $\alpha_1, \beta_1, \beta_2 \in \R$. We can represent the matrix $A$ with respect to the basis $\mathcal{B}$, denoting with $a_{ij} = \textbf{e}^T_i A \textbf{e}_j $  the element of the $i-th$ row and $j-th$ column of the matrix $A$ written in the basis $\mathcal{B}$. We can represent the matrix $ \textbf{u}\textbf{v}^T$ with respect to the basis $\mathcal{B}$, obtaining $\textbf{u}\textbf{v}^T = \alpha_1 \beta_1 \textbf{e}_1 \textbf{e}_1^T +  \alpha_1 \beta_2 \textbf{e}_1 \textbf{e}_2^T$. Knowing that we can compute the determinant of a matrix $M$ $n\times n$ using the formula:
\begin{equation}\label{key}
	\det(M) = \sum_{j=1}^{n} m_{ij} C_{ij}(M),
\end{equation}
where $C_{ij}$ is the cofactor of the element $(i,j)$ of the matrix $M$, the determinant of $\tilde{A}$ is:
\begin{equation}\label{key}
	\det(\tilde{A}) = \det(A) + \alpha_1 \beta_1 C_{11}(A) + \alpha_1 \beta_2 C_{12}(A).
\end{equation}
Since we know that $ \det(\tilde{A}) \neq 0$, we can write:
\begin{equation}\label{key}
	 \det(A) + \alpha_1 \beta_1 C_{11}(A) + \alpha_1 \beta_2 C_{12}(A) \neq 0,
\end{equation}
and, therefore:
\begin{equation}\label{key}
	 \alpha_1 \beta_1 \frac{C_{11}(A)}{\det(A)} + \alpha_1 \beta_2 \frac{C_{12}(A)}{\det(A)} \neq -1,
\end{equation}
where we divided for $\det(A)$ both sides of the equation, knowing that $\det(A) \neq 0$. At this point, it is straightforward to verify that 
\begin{equation}\label{key}
	\textbf{v}^T A^{-1} \textbf{u} = \alpha_1 \beta_1 \frac{C_{11}(A)}{\det(A)} + \alpha_1 \beta_2 \frac{C_{12}(A)}{\det(A)}
\end{equation}
writing $\textbf{u}$ and $\textbf{v}$ in terms of $\textbf{e}_1$ and $\textbf{e}_2$ and $A^{-1} = \frac{1}{\det(A)} (cof(A))^T$, where $cof(A)$ is the matrix of cofactors of $A$. This proves that:
\begin{equation}\label{key}
	\textbf{v}^T A^{-1} \textbf{u} \neq -1.
\end{equation}

\noindent In order to prove the converse implication, we consider again the expression for the determinant of $\tilde{A}$:
\begin{equation}\label{key}
	\det(\tilde{A}) = \det(A) + \alpha_1 \beta_1 C_{11}(A) + \alpha_1 \beta_2 C_{12}(A) = \det(A) \left( 1+ \alpha_1 \beta_1 \frac{C_{11}(A)}{\det(A)} + \alpha_1 \beta_2 \frac{C_{12}(A)}{\det(A)}\right).
\end{equation}
Here we can recognize the expression of $\textbf{v}^T A^{-1} \textbf{u}$, obtaining:
\begin{equation}\label{key}
	\det(\tilde{A}) = \det(A) (1 + \textbf{v}^T A^{-1} \textbf{u}).
\end{equation}
Now, knowing that $\det(A)\neq 0 $ and $\textbf{v}^T A^{-1} \textbf{u} \neq -1$, we obtain
\begin{equation}\label{key}
	\det(\tilde{A}) \neq 0
\end{equation}
and this concludes the proof.\\
\\
\noindent \textbf{(1b)} Show that:
\begin{equation}\label{eq: A tilde ^-1}
	\tilde{A}^{-1} = A^{-1}  - \alpha A^{-1} \textbf{u}\textbf{v}^T A^{-1},\ \ \ \text{where } \alpha = \frac{1}{\textbf{v}^T A^{-1}\textbf{u} + 1}.
\end{equation}
Proof: We start noticing that the last expression is well defined since $\tilde{A}$ invertible implies $\textbf{v}^T A^{-1} \textbf{u} + 1 \neq 0$. Now we can manipulate the \eqref{eq: A tilde ^-1} multiplying both sides to the left for $A$ and to the right for $\tilde{A}$, obtaining:
\begin{equation}\label{key}
\begin{split}
	A &= \tilde{A} - \alpha \textbf{u} \textbf{v}^T A^{-1} \tilde{A} \\
	&= A + \textbf{u}\textbf{v}^T - \alpha \textbf{u} \textbf{v}^T A^{-1} (A + \textbf{u}\textbf{v}^T )\\
	&= A + (1-\alpha) \textbf{u}\textbf{v}^T - \alpha \textbf{u}\textbf{v}^TA^{-1} \textbf{u}\textbf{v}^T.
\end{split}
\end{equation}
Subtracting $A$ from each side and dividing both sides for $\alpha$ (that is nonzero $\forall \textbf{v}^T A^{-1}\textbf{u} \in \R$) we obtain:
\begin{equation}\label{eq: identity theorem 1b}
	\textbf{u}\textbf{v}^T A^{-1} \textbf{u}\textbf{v}^T = \textbf{u}\textbf{v}^T(\alpha^{-1}-1).
\end{equation}
Finally, since $\textbf{v}^T A^{-1} \textbf{u} = \alpha^{-1} - 1$, the identity \eqref{eq: identity theorem 1b} is verified and this concludes the proof.\\
\\
\textbf{(1c)} Assuming that LU factorization of $A$ is already available, describe an $\mathcal{O}(n^2)$ algorithm to solve $\tilde{A} \tilde{\textbf{x}} = \tilde{\textbf{b}}$ for any right-hand side $\tilde{\textbf{b}}$.\\
Supposing that $\tilde{A}$ is invertible, we can write the solution $\tilde{\textbf{x}}$ using the Sherman-Morrison formula for $\tilde{A}$:
\begin{equation}\label{key}
	\tilde{\textbf{x}} = \tilde{A}^{-1} \tilde{\textbf{b}} =  A^{-1} \tilde{\textbf{b}}  - \frac{A^{-1}\textbf{u}\textbf{v}^T A^{-1}\tilde{\textbf{b}} }{\textbf{v}^T A^{-1} \textbf{u} + 1 }
\end{equation}

\noindent Algorithm: 
\begin{itemize}
	\item Compute \textbf{x} s.t. $A\textbf{x} = \tilde{\textbf{b}}$ and \textbf{y} s.t. $A\textbf{y} = \textbf{u}$ using backward and forward substitutions. This requires $\mathcal{O}(n^2)$ operations.
	\item Compute $\gamma = \frac{\textbf{v}^T \textbf{x}}{\textbf{v}^T \textbf{y} + 1}$. This requires $\mathcal{O}(n)$ operations.
	\item Compute $\tilde{\textbf{x}} = \textbf{x} - \gamma \textbf{y}$. This requires $\mathcal{O}(n)$ operations.
\end{itemize}


\noindent \textbf({2)} Assuming again that the LU factorization of $A$ existes and has been computed, describe an efficient algorithm for solving the \textit{bordered system}
\begin{equation}\label{key}
\begin{bmatrix}
	A &  \textbf{u} \\ 
	\textbf{v}^T & \beta 
\end{bmatrix}
\begin{bmatrix}
\textbf{x} \\ 
z
\end{bmatrix}
=
\begin{bmatrix}
	\textbf{b} \\ 
	c
\end{bmatrix},
\end{equation}
where $z$ is unknown and $\beta$ and $c$ are given scalars. When does this system have a unique solution?

\noindent Solution: \\
Putting 
\begin{equation}\label{key}
	A' = 
\begin{bmatrix}
	A &  \textbf{u} \\ 
	\textbf{v}^T & \beta 
\end{bmatrix}, 
\textbf{x}' = \begin{bmatrix}
	\textbf{x} \\ 
	z
\end{bmatrix}, 
\textbf{b}' = 
\begin{bmatrix}
	\textbf{b} \\ 
	c
\end{bmatrix},
\end{equation}
the system above rewrites as:
\begin{equation}\label{key}
	A' \textbf{x}' = \textbf{b}'.
\end{equation}
The LU factorization of the matrix $A' = L' U'$ exists and the matrices $L'$ and $U'$ take the following form:
\begin{equation}\label{key}
	L' = 
	\begin{bmatrix}
		L &  \textbf{0} \\ 
		\textbf{f}^T & 1 
	\end{bmatrix}, 	
U' = 
\begin{bmatrix}
U&  \textbf{g} \\ 
\textbf{0} & \gamma
\end{bmatrix}.
\end{equation}
In order to get the values of $\textbf{f}, \textbf{g}\in \R^n$ and $\gamma \in \R$, we impose $ A' = L' U'$, obtaining the following system:
\begin{equation}\label{key}
	\left\lbrace 
	\begin{split}
		& L\textbf{g} = \textbf{u}\\
		& U^T \textbf{f} = \textbf{v}\\
		&\textbf{f}^T \textbf{g} + \gamma = \beta
	\end{split} \right.  .
\end{equation}
Here we can find $\textbf{f}$ and $\textbf{g}$ with forward substitutions with $\mathcal{O}(n^2)$ operations. Therefore, we can rewrite the last equation as:
\begin{equation}\label{key}
	 \gamma = \beta - \textbf{v}^T A^{-1} \textbf{u} .
\end{equation}
In order to impose that the bordered system has a unique solution, we have to require that $\det(A') \neq 0$, that is true if and only if all the diagonal elements of $U'$ are nonzero and, given that $\det(A)\neq 0$, this means requiring that $\gamma \neq 0 $. Therefore, the condition for the uniquenes of the solution becomes:
\begin{equation}\label{key}
	\gamma = \beta - \textbf{v}^T A^{-1} \textbf{u} \neq 0 \Rightarrow \ \ \textbf{v}^T A^{-1} \textbf{u} \neq \beta.
\end{equation}


	
\end{document} 