\documentclass[a4paper,11pt]{article}
%\usepackage{minted}
%\usepackage{pygmentize}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subfig}
\usepackage{geometry}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{bbold}
\usepackage{mathtools}
\usepackage{braket}
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage[english]{babel}
\usepackage{lipsum}
\usepackage{setspace}
\usepackage{xcolor}
\newcommand{\R}{\mathbb{R}}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue}
\geometry{a4paper, top=2.5cm, bottom=2.5cm, left=3cm, right=2.5cm}

\begin{document}
\author{Catalano Giuseppe, Cerrato Nunzia}
\title{Numerical Linear Algebra Homework Project 2:\\Least Squares, Orthogonalization, and the SVD}
\date{}
\maketitle

\section*{Problem 1}
\textbf{(1)} Suppose we are given $m$ pairs of data points, $(x_1,y_1),\dots,(x_m,y_m)$. We want to find a linear combination of prescribed functions $\phi_1, \dots, \phi_n$ whose values at the points $x_i \in [a,b] $, $1\le i \le m$, approximate the values $y_1,\dots, y_m$ as well as possible. More precisely, the problem is to find a function of the form $f(x) = \alpha_1 \phi_1(x) + \dots + \alpha_n \phi_n(x) $ such that 
\begin{equation}
	\sum_{i=1}^{m} \left[ y_i - f(x_i) \right]^2 \le \sum_{i=1}^{m} \left[ y_i - g(x_i) \right]^2\ \ \ \forall g \in \text{Span}(\phi_1,\dots,\phi_n),
\end{equation}
where, usually, $m>n$. It is possible to rephrase the problem as:
\begin{equation}\label{eq: initial definition of the minimization problem}
	f = \arg \min_{f\in \text{Span}(\phi_1,\dots,\phi_n)} \sum_{i=1}^{m} \left[ y_i - f(x_i) \right]^2.
\end{equation}
Now we can define a column vector $\textbf{z} \in \R^n$ such that:
\begin{equation}\label{key}
	[\textbf{z}]_i = \alpha_i
\end{equation}
and a matrix $A$ such that:
\begin{equation}\label{key}
	[A\textbf{z}]_i = f(x_i) = \alpha_1 \phi_1(x_i) + \dots + \alpha_n \phi_n(x_i).
\end{equation}
In this way, the element of the $i-th$ row and $j-th$ column of the matrix $A$ is:
\begin{equation}\label{key}
	[A]_{ij} = a_{ij} = \phi_j(x_i).
\end{equation}
Finally, defining a column vector $\textbf{b} \in \R^n$ such that:
\begin{equation}\label{key}
	[\textbf{b}]_i = y_i
\end{equation}
we can rewrite the \eqref{eq: initial definition of the minimization problem} as follows:
\begin{equation}\label{key}
	\tilde{\textbf{z}} = \arg \min_{\textbf{z}\in \R^n} \lVert \textbf{b} - A \textbf{z} \rVert_2^2 =  \arg \min_{\textbf{z}\in \R^n} \lVert \textbf{b} - A \textbf{z} \rVert_2,
\end{equation}
where the function $f$ can be built from $\tilde{\textbf{z}}$.

\noindent \textbf{(2)} Now we suppose to take $\phi_k = x^{k-1}$, $1 \le k \le n$. Under this assumption, the matrix $A$ takes the form:
\begin{equation}\label{key}
	A = \begin{bmatrix}
		x_1^0 & \cdots & x_1^{n-1}  \\
		\vdots & \ddots & \vdots \\
		x_m^0 & \cdots  & x_m^{n-1}
	\end{bmatrix}.
\end{equation}
We want to prove that, assuming that $x_i\neq x_j$ for $i \neq j$, $A$ has full rank: $\text{rank}(A) = n$.\\
Proof: Proving that $\text{rank}(A) = n$ is equivalent to prove that $\dim(\ker(A)) = 0$, that means that $\nexists \textbf{v} \in \R^n \text{ s.t. } \textbf{v} \in \ker(A)$. We want to prove this statement by contraddiction, therefore, we look for a vector $\textbf{v} \in \R^n$, with $\textbf{v} \neq \underline{0}$, such that $A\textbf{v}=\underline{0}$, that means:
\begin{equation}\label{key}
	\left\lbrace 
	\begin{split}
		&  v_1 x_1^0 + \dots + v_n x_1^{n-1} = 0\\
		& \vdots  \\
		&  v_1 x_m^0 + \dots + v_n x_m^{n-1} = 0 
	\end{split} \right.  .
\end{equation}
Defining the polynomial 
\begin{equation}\label{key}
	p^{(n-1)}_{\textbf{v}}(x) = \sum_{i=1}^{n} v_i x^{i-1}
\end{equation}
we can observe that, for any choice of $\textbf{v} \neq \underline{0}$, $p^{(n-1)}_{\textbf{v}}(x)$ admits at most $n-1$ different roots, therefore $\nexists \textbf{v} \neq \underline{0}$ such that $A\textbf{v}=\underline{0}$. This concludes the proof. \qedsymbol

\noindent \textbf{(3)} Consider the problem of finding the best fit with a quadratic function $f(x) = \alpha_1 + \alpha_2 x + \alpha_3 x^2$ for the following data:\\
\begin{center}	
\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
	$x_i$& 8 & 10 & 12 & 16 & 20 & 30 & 40 & 60 & 100 \\
	\hline
	$y_i$& 0.88 & 1.22 & 1.64 & 2.72 & 3.96 & 7.66 & 11.96 & 21.56 & 43.16 \\
\end{tabular}.
\end{center}
In the following we report the code that solves the normal equations:
\begin{equation}\label{key}
	A^T A \textbf{v} = A^T \textbf{b}
\end{equation}
using the Cholesky factorization algorithm and then compares the result with the one found using the QR factorization of the matrix $A$.

\noindent *** INSERIRE CODICE ***

\noindent Using the previous functions we have computed the solution to the minimization problem, obtaining the following results:\\
\begin{tabular}{cc}
	Cholesky:  &$x = [-1.91914925269909,\   0.278213536291725,\ 0.001739400875055]$\\
	QR factorization:&$x = [-1.91914925269904,\  0.278213536291722,\ 0.001739400875055]$ \\
\end{tabular}.

\noindent From these results we can observe that, for this problem, both the algorithms perform in a similar way. In fact, the results differ at most in the 15th digit. In figure \ref{fig: fit and data Cholesky QR} we show the input data and the solutions to the least square problem.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.1\linewidth]{../Project_2_latex/Plot/Fit_and_data.png}
	\caption{}
	\label{fig: fit and data Cholesky QR}
\end{figure}



\noindent \textbf{(4)} The following code computes the residual $\textbf{r} = \textbf{d} - C\hat{\textbf{x}}$ where $\hat{\textbf{x}} =  \left[  - 1.919,\  0.2782,\  0.001739\right] $ is the approximate solution of the least squares problem.

\noindent ***INSERIRE CODICE***\\

\noindent The results that we obtain are:\\
\begin{tabular}{cc}
 	Residual: &$\textbf{r}  = [ 0.009503999999993,\  0.716895999999906,\ 62.090847999905236]$\\
 	Norm 2 of the residual: &$\lVert  \textbf{r}  \rVert_2 = 62.09498720144942$\\
\end{tabular}.\\
The value of the residual may seem strange, in fact, if we compute the relative error we find:
\begin{equation}\label{key}
	\frac{\lVert  \textbf{x} - \hat{\textbf{x}} \rVert_2 }{\lVert  \textbf{x} \rVert_2 }  = 7.728184292875672\text{e-05}.
\end{equation}

\noindent However, we can observe that, from the relation
\begin{equation}\label{key}
	\frac{1}{k_2(C)} \frac{\lVert  \textbf{r}  \rVert_2 }{\lVert  \textbf{d}  \rVert_2 } \le \frac{\lVert  \textbf{x} - \hat{\textbf{x}} \rVert_2 }{\lVert  \textbf{x} \rVert_2 } \le k_2(C) \frac{\lVert  \textbf{r}  \rVert_2 }{\lVert  \textbf{d}  \rVert_2 }
\end{equation}
we can obtain the following relation for $\textbf{r} $:
\begin{equation}\label{key}
	\frac{\lVert  \textbf{d}  \rVert_2}{k_2(C)} \frac{\lVert  \textbf{x} - \hat{\textbf{x}} \rVert_2 }{\lVert  \textbf{x} \rVert_2 }\le \lVert  \textbf{r}  \rVert_2 \le k_2(C) \lVert  \textbf{d}  \rVert_2 \frac{\lVert  \textbf{x} - \hat{\textbf{x}} \rVert_2 }{\lVert  \textbf{x} \rVert_2 }.
\end{equation}

\noindent With the following python code we have computed the bounds to the residual.
*** INSERIRE CODICE***\\

\noindent From the output of the previous code we know that:
\begin{equation}\label{key}
	5 \times 10^{-7} \lessapprox \lVert  \textbf{r}  \rVert_2 \lessapprox 3 \times 10^9
\end{equation}
this big range is due to the value of $k_2(C)  \simeq 8 \times 10^7$ that suggests us that we should not use the residual to measure the accuracy of the solution when the problem is ill-conditioned as in this case. 

\section*{Problem 2}
\textbf{(1)} Let $A \in \R^{m\times n}$, with $\text{rank}(A) = n$, let {A = QR} be the (full) QR factorization of $A$, with $Q \in \R^{m\times m} $ orthogonal and $R \in \R^{m\times n}$ upper trapezoidal. Also, let $A = Q_1 R_1$ be the reduced QR factorization of $A$ with $Q_1 \in \R^{m\times n}$ having orthonormal columns and $R_1 \in \R^{m\times m}$ upper triangular. Show that $R_1$ is nonsingular, and that the columns $\textbf{q}_1 . . . , \textbf{q}_n$ of $Q_1$ form an orthonormal basis for $\text{Ran}(A)$, the column space of $A$. Also, find an orthonormal basis for $\text{Null}(A^T)$, the null space of $A^T$ .\\
We start showing that $R_1$ is nonsingular. Since $A$ has full rank, we know that:
\begin{equation}\label{key}
	A \textbf{x} = \textbf{0} \Leftrightarrow \textbf{x} = \textbf{0}
\end{equation}
and multiplying both sides for $Q_1^T$ knowing that $Q_1^T Q_1 = \mathcal{I}_n$ we obtain
\begin{equation}\label{key}
	R_1 \textbf{x} = \textbf{0} \Leftrightarrow \textbf{x} = \textbf{0},
\end{equation}
that concludes the proof.\\
Now we want to show that the columns $\textbf{q}_1 . . . , \textbf{q}_n$ of $Q_1$ form an orthonormal basis for $\text{Ran}(A)$. We start observing that 
\begin{equation}\label{key}
	\forall \textbf{y} \in \text{Ran}(A) \exists \textbf{x} \in \R^n : \textbf{y} = A \textbf{x} = Q_1R_1\textbf{x}
\end{equation}
and we know this from the definition of range of a matrix. In a similar way, knowing that $R_1$ is nonsingular and therefore is a bijective map from $\R^n$ to $\R^n$, we can put $\textbf{x}' = R_i \textbf{x}$ and say that:
\begin{equation}\label{key}
	\forall \textbf{y} \in \text{Ran}(A) \exists \textbf{x} \in \R^n : \textbf{y} = A \textbf{x} = Q_1\textbf{x}',
\end{equation}
that means that $\text{Ran}(A) = \text{Ran}(Q_1) = \text{Span}\{\textbf{q}_1,\dots,\textbf{q}_n\}$.\\
Now we want to find an orthonormal basis for $\text{Null}(A^T)$. Knowing that $\text{Null}(A^T) = (\text{Ran}(A))^\perp$ we can, immediately, find the solution. In fact:
\begin{equation}\label{key}
	(\text{Ran}(A))^\perp = \text{Span}\{\textbf{q}_1,\dots,\textbf{q}_n\}^\perp = \text{Span}\{\textbf{q}_{n+1}\dots,\textbf{q}_m\}.
\end{equation}
This concludes the proof. \qed\\

\noindent \textbf{(2)} Given tha full rank matrix
\begin{equation}\label{key}
	A = \begin{bmatrix}
		1.07 & 1.10  \\
		1.07 & 1.11  \\
		1.07 & 1.15 
	\end{bmatrix},
\end{equation}
compute $A^T A$ in $\beta = 10$, $t=3$ digit arithmetic and verify if $A^T A$ is positive definite.\\
We performed the prescribed calculation using the rounding function
\begin{equation}\label{key}
	\text{fl}(x) = \arg \min_{y\in \mathbb{F}} |y-x|
\end{equation}
where $\mathbb{F} = \{\pm(0.d_1d_2d_3)\times 10^p : d_i \in {0,\dots,9},\ -2046\le p \le 2046\}$ and, when there is ambiguity, we approximate always away from $0$. Moreover, the rounding function has been applied at each step of the calculation, meaning that, if we consider $\textbf{v}, \textbf{u} \in \R^n$
\begin{equation}\label{key}
	\textbf{v}^T \textbf{u} = \text{fl} \left( \sum_{i=1}^{n}\text{fl} (v_i u_i)\right) .
\end{equation}
The result of the calculation is the following:
\begin{equation}\label{key}
	A^T A =  \begin{bmatrix}
		3.42 & 3.6  \\
		3.6 & 3.76  \\
	\end{bmatrix}.
\end{equation}
The last matrix is indefinite, in fact $\text{Det}(A^T A) = 3.42 \times 3.76 - 3.6^2 = 12.8592 -  12.96 < 0$.\\

\noindent \textbf{(3)} Compute the QR factorization of the matrix
\begin{equation}\label{key}
	A = \begin{bmatrix}
		1.07 & 1.10  \\
		1.07 & 1.11  \\
		1.07 & 1.15 
	\end{bmatrix} = [\textbf{a}_1\ \textbf{a}_2]
\end{equation}
in 3 digit arithmetic, i.e. with $\beta = 10$, $t=3$. The $\text{fl}(x)$ function has been applied at each step, as in the previous exercise.\\
We want to compute $H_1 A$:
\begin{equation}\label{key}
	H_1 A = \begin{bmatrix}
				r_{11} & \textbf{r}_1^T  \\
		\textbf{0} & A_1  \\
	\end{bmatrix}.
\end{equation} 
\begin{equation}\label{key}
	r_{11} = -\text{sgn}(a_{11})\lVert \textbf{a}_1\rVert_2 = \lVert \textbf{a}_1\rVert_2 = \sqrt{1.14 \times 3 }= \sqrt{3.42} = -1.85
\end{equation}

\begin{equation}\label{key}
	\hat{\textbf{u}}_1  = \textbf{a}_1 + r_{11} \textbf{e}_1 = \begin{pmatrix}
		1.07\\
		1.07\\
		1.07
	\end{pmatrix} + 1.85 \begin{pmatrix}
	1\\
	0\\
	0
\end{pmatrix} = \begin{pmatrix}
2.92\\
1.07\\
1.07
\end{pmatrix}
\end{equation}

\begin{equation}\label{key}
	\lVert \hat{\textbf{u}} \rVert_2 = \sqrt{8.53 + 1.14 + 1.14} = \sqrt{10.81} = 3.29
\end{equation}

\begin{equation}\label{key}
		\textbf{u}_1 = 	\hat{\textbf{u}}_1/\lVert \hat{\textbf{u}} \rVert_2  =  \begin{pmatrix}
			0.888\\
			0.325\\
			0.325
		\end{pmatrix} 
\end{equation}

\begin{equation}\label{key}
	\begin{split}
		H_1 \textbf{a}_2 &= \textbf{a}_2 - 2 (\textbf{u}_1^T\textbf{a}_2) \textbf{u}_1  \\
		&= \begin{pmatrix}
			1.10\\
			1.11\\
			1.15
		\end{pmatrix} - 2(0.977 + 0374 + 0.374) \begin{pmatrix}
		0.888\\
		0.325\\
		0.325
	\end{pmatrix} \\
	&= \begin{pmatrix}
		1.10\\
		1.11\\
		1.15
	\end{pmatrix} - 3.42 \begin{pmatrix}
		0.888\\
		0.325\\
		0.325
	\end{pmatrix} = \begin{pmatrix}
	-1.94\\
	0\\
	0.04
\end{pmatrix}
	\end{split} .
\end{equation}
Therefore we have:
\begin{equation}\label{key}
	H_1 A = \begin{pmatrix}
		-1.85& -1.94 \\
		0& 0  \\
		0& 0.04
	\end{pmatrix}. 
\end{equation}
Now we iterate the same procedure on $A_1$, with
\begin{equation}\label{key}
	A_1 = [\textbf{a}_2^{(1)}] = \begin{pmatrix}
		0\\
		0.04
	\end{pmatrix}.
\end{equation}


\begin{equation}\label{key}
	\hat{\textbf{u}}_2^{(1)} = \textbf{a}_2^{(1)} + \text{sgn}(a_{12}^{(1)}) \lVert \textbf{a}_2^{(1)} \rVert_2\textbf{e}_2^{(1)} = \begin{pmatrix}
		0\\
		0.04
	\end{pmatrix}+ \text{sgn}(0) 0.04 \begin{pmatrix}
		1\\
		0
	\end{pmatrix} = \begin{pmatrix}
		0.04\\
		0.04
	\end{pmatrix}
\end{equation}


\begin{equation}\label{key}
	\textbf{u}_2^{(1)} = \hat{\textbf{u}}_2^{(1)}/ \lVert \hat{\textbf{u}}_2 ^{(1)}\rVert_2 = \begin{pmatrix}
		0.707\\
		0.707
	\end{pmatrix}.
\end{equation}

\noindent The complete vector $\textbf{u}_2$ will be:
\begin{equation}\label{key}
	\textbf{u}_2 = \begin{pmatrix}
		0\\
		0.707\\
		0.707
	\end{pmatrix}
\end{equation}
and the element $r_{22}$ willl be
\begin{equation}\label{key}
	r_{22} = - \text{sgn}(a_{12}^{(1)}) \lVert \textbf{a}_2^{(1)} \rVert_2 = -0.04
\end{equation}
so that, finally, we have
\begin{equation}\label{key}
	H_2 H_1 A = R = \begin{pmatrix}
		-1.85& -1.94 \\
		0& -0.04  \\
		0& 0
	\end{pmatrix}.
\end{equation}
We can observe that the matrix $R$ is full rank, therefore it is possible to use the QR factorization to solve the least square problem. It is important to stress that, as $A^T A$ is indefinite in $3$ digit precision, using Cholesky is not possible.\\
Now we want to compute $Q=[\textbf{q}_1,\textbf{q}_2,\textbf{q}_3]$ and, in order to do it, we use the following relations:
\begin{equation}\label{key}
	\begin{split}
		&\textbf{q}_1 = H_1 H_2 \textbf{e}_1 = \textbf{e}_1 - 2 (\textbf{u}_1^T \textbf{e}_1) \textbf{u}_1\\
		&  \textbf{q}_2 = H_1 H_2 \textbf{e}_2 = H_1 (\textbf{e}_2 - 2 (\textbf{u}_2^T \textbf{e}_2) \textbf{u}_2)  \\
		&=  \textbf{e}_2 - 2 (\textbf{u}_2^T \textbf{e}_2) \textbf{u}_2 -  2 (\textbf{u}_1^T \textbf{e}_2) \textbf{u}_1  + 4 (\textbf{u}_2^T \textbf{e}_2)(\textbf{u}_1^T \textbf{u}_2)\textbf{u}_1\\
		&  \textbf{q}_3 = H_1 H_2 \textbf{e}_3 = H_1 (\textbf{e}_3 - 2 (\textbf{u}_2^T \textbf{e}_3) \textbf{u}_2)  \\
		&=  \textbf{e}_3 - 2 (\textbf{u}_2^T \textbf{e}_3) \textbf{u}_2 -  2 (\textbf{u}_1^T \textbf{e}_3) \textbf{u}_1  + 4 (\textbf{u}_2^T \textbf{e}_3)(\textbf{u}_1^T \textbf{u}_2)\textbf{u}_1
	\end{split}.
\end{equation}
Using these relations, we found the approximated matrix $Q$, that is:
\begin{equation}\label{key}
	Q = \begin{bmatrix}
		-0.578& 0.572 & 0.572 \\
		-0.578& 0.212 & -0.788 \\
		-0.578& -0.788 & 0.212
	\end{bmatrix}.
\end{equation}
Finally, we can verify that, multiplying $Q$ and $R$ and approximating as before, we recover the original matrix $A$. It is important to point out that the columns of $Q$ are not perfectly orthonormal.\\

\noindent \textbf{(4)} Let $A \in \R^{m\times n}$, with $\text{rank}(A) = n$, and let $\textbf{b} \in \R^m$. Let $A = Q_1R_1$ be a reduced QR decomposition of $A$, where $Q_1 \in \R^{m\times n}$ has orthonormal columns and $R_1 \in \R^{n\times n}$ is upper triangular and nonsingular. Show that a reduced QR factorization of the augmented matrix $A_+ = [ A\ \textbf{b} ]$ is given by:
\begin{equation}\label{key}
	A_+=[Q_1\  \textbf{q}_{n+1}] \begin{bmatrix}
		R_1& \textbf{z}  \\
		\textbf{0}^T& \rho 
	\end{bmatrix}
\end{equation}
where $\textbf{z} = Q_1^T\textbf{b}$. Also, show that $|\rho| = \lVert \textbf{b} - A \textbf{x}^*\rVert_2$ where $\textbf{x}^*$ is the solution to the least squares problem $\lVert \textbf{b} - A \textbf{x}\rVert_2= \text{min} $.\\
We start recalling that any matrix has a QR factorization, therefore the matrix $A_+$ must have it. In particular we know that $A_+ = Q_+ R_+$, where $Q_+$ has orthonormal columns and $R_+$ is upper triangular. In order to find the value of $\textbf{z}$ and $\rho$, we compute the product $Q_+ R_+$ and impose that it is equal to $A_+$:
\begin{equation}\label{key}
	[Q_1\  \textbf{q}_{n+1}] \begin{bmatrix}
		R_1& \textbf{z}  \\
		\textbf{0}^T& \rho 
	\end{bmatrix} = [Q_1 R_1\ Q_1\textbf{z} + \rho \textbf{q}_{n+1}] = [A\ \textbf{b}],
\end{equation}
therefore we must have 
\begin{equation}\label{key}
	\textbf{b} = Q_1 \textbf{z} + \rho \textbf{q}_{n+1}.
\end{equation}
Multiplying both members of the last equation for $Q_1^T$ we find that:
\begin{equation}\label{key}
	Q_1^T \textbf{b} = Q_1^T Q_1 \textbf{z} +  \rho Q_1^T \textbf{q}_{n+1} = \textbf{z}
\end{equation}
where we used that the columns of $Q_1$ are orthonormal and $\textbf{q}_{n+1}$ is orthogonal to all the columns of $Q_1$.\\
Now we consider the complete QR factorization of $A_+$:
\begin{equation}
	A_+ =QR= [Q_1\ Q_2] \left[ \begin{tabular}{c}
		\begin{tabular}{cc}
			$R_1$& $\textbf{z}$ \\
			$\textbf{0}^T$& $\rho$ \\
		\end{tabular}\\
	\hline
		$\textbf{0}$
	\end{tabular}\right] = [Q_1 R_1\ \ Q_1 \textbf{z} + Q_2(\rho,0,\dots,0)^T  ]
\end{equation}
obtaining the relation
\begin{equation}\label{key}
	\textbf{b} = Q_1 \textbf{z} + Q_2(\rho,0,\dots,0)^T
\end{equation}
and, therefore, multiplying both sides for $Q_2^T$:
\begin{equation}\label{key}
	Q_2^T \textbf{b} = \rho (1,0,\dots,0)^T.
\end{equation}

\noindent Now we define the residual $\textbf{r}$ as
\begin{equation}\label{key}
	\textbf{r} = \textbf{b} - A \textbf{x}^*
\end{equation}
where $ \textbf{x}^*$ is the solution to the least square problem, and compute:
\begin{equation}\label{key}
	\lVert \textbf{r} \rVert_2^2 = \lVert Q^T \textbf{r} \rVert_2^2 = \lVert \begin{bmatrix}
		Q_1^T \textbf{b}\\
		Q_2^T \textbf{b}
	\end{bmatrix} -\begin{bmatrix}
		Q_1^T A \textbf{x}^*\\
		Q_2^T A \textbf{x}^*
	\end{bmatrix} \rVert_2^2 = \lVert Q_1^T \textbf{b} - R_1 \textbf{x}^* \rVert_2^2 + \lVert Q_2^T \textbf{b} \rVert_2^2
\end{equation}
where we used the invariance of the 2-norm with respect to unitary transformations. Knowing that $\textbf{x}^* = R_1^{-1} Q_1^T \textbf{b}$ we obtain:
\begin{equation}\label{key}
	\lVert \textbf{r} \rVert_2 = \lVert Q_2^T \textbf{b} \rVert_2 = \lVert \rho (1,0,\dots,0)^T \rVert_2 = |\rho|
\end{equation}
that concludes the proof. \qed

\section*{Problem 3}

\noindent \textbf{(1)} Let $A \in \R^{m\times n}$, with singular value decomposition $A = U \Sigma V^T = \sum_{i=1}^n \sigma_i \textbf{u}_i \textbf{v}_i^T$ with $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_n$ and $\text{rank}(A) = n$. Express the singular values and singular vectors of the following matrices in terms of those of $A$.\\

\textbf{(a)} $(A^{T}A)^{-1}$
\[(A^{T}A)^{-1} = (V^{T})^{-1}\Sigma^{-2}V^{-1}=V\Sigma^{-2}V^{-1}\]
%From this expression we can conclude that:
\begin{itemize}
	\item Both left and right singular vectors of $(A^{T}A)^{-1}$ are equal to the right singular vectors of $A$;
	\item Singular values of $(A^{T}A)^{-1}$ are equal to singular values of $A$ raised to the $-2$ power.
\end{itemize}

\textbf{(b)} $(A^{T}A)^{-1}A^{T}$
\[(A^{T}A)^{-1}A^{T} = V\Sigma^{-2}V^{T}V\Sigma U^{T}=V\Sigma^{-1}U^{-1}\]
\begin{itemize}
	\item Left singular vectors of $(A^{T}A)^{-1}A^{T}$ are equal to the right singular vectors of $A$;
	\item Right singular vectors of $(A^{T}A)^{-1}$ are equal to the left singular vectors of $A$;
	\item Singular values of $(A^{T}A)^{-1}$ are equal to the inverse of singular values of $A$.
\end{itemize}

\textbf{(c)} $A(A^{T}A)^{-1}$
\[A(A^{T}A)^{-1} = U\Sigma V^{T}V\Sigma^{-2}V^{-T}=U\Sigma^{-1}V^{T}\]
\begin{itemize}
	\item Left and right singular vectors of $(A^{T}A)^{-1}$ coincide with that of $A$;
	\item Singular values of $(A^{T}A)^{-1}$ are equal to the inverse of singular values of $A$.
\end{itemize}

\textbf{(d)} $A(A^{T}A)^{-1}A^{T}$
\[A(A^{T}A)^{-1}A^{T} = U\Sigma (V^{T})V\Sigma^{-2}V^{T}V\Sigma U^{T}=\mathbb{1}\]



\noindent \textbf{(2)}

\begin{equation}
	A = \begin{bmatrix}
		1 & 2 \\
		0 & 2
	\end{bmatrix}
\end{equation}

\begin{equation}
	A^{T}A = \begin{bmatrix}
		1 & 2 \\
		2 & 8
	\end{bmatrix}
\end{equation}

\begin{equation}
	\det(A^{T}A -\lambda\mathbb{1})=0
\end{equation}

\begin{equation}
	\lambda_{1,2}=\frac{9 \pm \sqrt{65}}{2}
\end{equation}

\begin{equation}
	k_{2}(A) = \frac{\sigma_{\max}}{\sigma_{\min}} = \sqrt{\frac{9+\sqrt{65}}{9-\sqrt{65}}}
\end{equation}


\end{document}